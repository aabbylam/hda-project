/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  warnings.warn(
/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: 
1080 fits failed out of a total of 3240.
The score on these train-test partitions for these parameters will be set to nan.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
553 fits failed with the following error:
Traceback (most recent call last):
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/model_selection/_validation.py", line 866, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 1389, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/pipeline.py", line 662, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 1382, in wrapper
    estimator._validate_params()
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 436, in _validate_params
    validate_parameter_constraints(
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 98, in validate_parameter_constraints
    raise InvalidParameterError(
sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.

--------------------------------------------------------------------------------
527 fits failed with the following error:
Traceback (most recent call last):
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/model_selection/_validation.py", line 866, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 1389, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/pipeline.py", line 662, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 1382, in wrapper
    estimator._validate_params()
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 436, in _validate_params
    validate_parameter_constraints(
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 98, in validate_parameter_constraints
    raise InvalidParameterError(
sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.

  warnings.warn(some_fits_failed_message, FitFailedWarning)
/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan 0.40844741 0.40966251 0.41253274
 0.40930232 0.41090328 0.41244416 0.41165392 0.41030252 0.4126441
 0.40947998 0.40895958 0.41098259 0.41058067 0.40987119 0.41121588
 0.40904459 0.40803892 0.40989476 0.40678878 0.40554284 0.40721725
 0.40678878 0.40554284 0.40721725 0.40568723 0.40540412 0.4065127
 0.38121796 0.38129154 0.38228526 0.38179038 0.38120001 0.38247014
 0.38353002 0.38081733 0.38121283 0.37943596 0.37842394 0.3781699
 0.37786868 0.37629517 0.37674234 0.37741593 0.37445977 0.37550401
 0.37141071 0.36772431 0.36743936 0.37141071 0.36772431 0.36743936
 0.3719167  0.36827286 0.36843742        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
 0.31261518 0.29916837 0.30089305 0.3117062  0.29950984 0.30135673
 0.31237774 0.30076814 0.30105331 0.31130707 0.29881024 0.3009547
 0.31075162 0.29925474 0.30111111 0.31212163 0.30070849 0.30098029
 0.31204136 0.30068531 0.30105279 0.31204136 0.30068531 0.30105279
 0.31215249 0.30071388 0.30113647 0.25369983 0.24183871 0.23757583
 0.25310161 0.24109723 0.23730444 0.25105007 0.23959565 0.2365776
 0.25499849 0.24172609 0.23743349 0.25385376 0.24065331 0.23699985
 0.25102308 0.23898432 0.23624053 0.25153475 0.23991065 0.23668916
 0.25153475 0.23991065 0.23668916 0.25090566 0.23937007 0.23639579
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan 0.38725636 0.38184929 0.38324928
 0.38561835 0.38345552 0.38450086 0.38637439 0.3818333  0.38295821
 0.38693595 0.38307594 0.383581   0.38587886 0.38133981 0.38301347
 0.38624253 0.38221672 0.38369951 0.38466871 0.38175125 0.3829738
 0.38466871 0.38175125 0.3829738  0.3853151  0.38107694 0.38179494
 0.33802911 0.33214307 0.33177378 0.33422014 0.32970009 0.32951181
 0.33841215 0.33055973 0.32978231 0.33836877 0.33081956 0.3294716
 0.3349563  0.32953986 0.32997184 0.3356439  0.32930604 0.32874589
 0.3361015  0.3294021  0.32973296 0.3361015  0.3294021  0.32973296
 0.33444623 0.32817394 0.32869514        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
 0.40776067 0.40895453 0.41095578 0.41126218 0.41032644 0.41204566
 0.41085069 0.41006892 0.41136028 0.40883441 0.4078087  0.40973916
 0.40674507 0.40724876 0.40935131 0.4106528  0.40858909 0.40987935
 0.40489789 0.4044015  0.40721227 0.40489789 0.4044015  0.40721227
 0.40748498 0.40540981 0.40653347 0.37917115 0.37765252 0.37970122
 0.38040349 0.37870027 0.37903379 0.37852386 0.37852638 0.37780714
 0.37627484 0.37483152 0.37538973 0.37696102 0.37515421 0.37552362
 0.37470527 0.37290656 0.37348367 0.36899116 0.36644578 0.36673086
 0.36899116 0.36644578 0.36673086 0.37060519 0.36813778 0.36750281
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan 0.40810572 0.40929877 0.41098793
 0.40858544 0.41012027 0.41303668 0.41341429 0.41299681 0.41358469
 0.40941197 0.41107204 0.41228519 0.41080895 0.41136819 0.4122579
 0.41062739 0.41172354 0.41303868 0.41036024 0.41063221 0.41052928
 0.41036024 0.41063221 0.41052928 0.4100008  0.40954933 0.410714
 0.38330262 0.38441821 0.38672956 0.38820966 0.38809716 0.38859873
 0.38794967 0.38557612 0.38651403 0.38294383 0.38159029 0.38305541
 0.38509625 0.38251798 0.38318509 0.3857982  0.38383692 0.38291119
 0.37836192 0.37534131 0.37566188 0.37836192 0.37534131 0.37566188
 0.37873186 0.37601899 0.37543159        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
 0.30790726 0.301018   0.30099671 0.30881586 0.30145797 0.30178139
 0.30785957 0.30143152 0.30156574 0.3077689  0.30194411 0.30140263
 0.30931443 0.30204245 0.30189702 0.3073496  0.30170771 0.3016111
 0.30811329 0.30206239 0.30180396 0.30811329 0.30206239 0.30180396
 0.30792459 0.30208458 0.30170145 0.25549887 0.24372741 0.24104966
 0.25623214 0.24386432 0.2399241  0.25556114 0.24398183 0.24041611
 0.25459035 0.24360074 0.23950653 0.25511069 0.24354298 0.23947492
 0.2546703  0.24400462 0.24005095 0.25643617 0.24454115 0.24023907
 0.25643617 0.24454115 0.24023907 0.25441497 0.2442191  0.24062269
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan 0.38825038 0.38490612 0.38537593
 0.38856986 0.38508249 0.38515148 0.38768273 0.3851429  0.38564725
 0.38870284 0.38397853 0.38553252 0.38647089 0.38545687 0.38596945
 0.38628174 0.38418522 0.3855533  0.38850048 0.38494347 0.38553142
 0.38850048 0.38494347 0.38553142 0.38658909 0.38246988 0.3853402
 0.33913897 0.33356478 0.33308499 0.34058473 0.33426653 0.33322036
 0.34233494 0.33420222 0.33165659 0.34003657 0.33453972 0.33262354
 0.33935123 0.3311854  0.33190351 0.33979481 0.33280918 0.33239094
 0.33631606 0.33230774 0.33071806 0.33631606 0.33230774 0.33071806
 0.33828691 0.33073831 0.33090241        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
 0.40968052 0.40909562 0.41208491 0.41042987 0.41094089 0.41303244
 0.41432367 0.41448145 0.41444993 0.41035028 0.41072338 0.41169246
 0.41264445 0.41213329 0.4125352  0.41115194 0.41060093 0.41180684
 0.41132034 0.40991805 0.41069296 0.41132034 0.40991805 0.41069296
 0.41001374 0.40974328 0.41003335 0.3823965  0.38186287 0.38426177
 0.38233292 0.38310477 0.38362018 0.38484659 0.38140699 0.38304651
 0.38113423 0.37984364 0.38003594 0.38362386 0.38210261 0.3816099
 0.38070706 0.3790013  0.3784083  0.37784538 0.37350754 0.37384053
 0.37784538 0.37350754 0.37384053 0.37861102 0.37501808 0.37434883]
  warnings.warn(
/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  warnings.warn(
/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: 
1080 fits failed out of a total of 3240.
The score on these train-test partitions for these parameters will be set to nan.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
533 fits failed with the following error:
Traceback (most recent call last):
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/model_selection/_validation.py", line 866, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 1389, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/pipeline.py", line 662, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 1382, in wrapper
    estimator._validate_params()
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 436, in _validate_params
    validate_parameter_constraints(
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 98, in validate_parameter_constraints
    raise InvalidParameterError(
sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.

--------------------------------------------------------------------------------
547 fits failed with the following error:
Traceback (most recent call last):
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/model_selection/_validation.py", line 866, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 1389, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/pipeline.py", line 662, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 1382, in wrapper
    estimator._validate_params()
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 436, in _validate_params
    validate_parameter_constraints(
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 98, in validate_parameter_constraints
    raise InvalidParameterError(
sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.

  warnings.warn(some_fits_failed_message, FitFailedWarning)
/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan 0.40662663 0.40850799 0.41115116
 0.40674297 0.40859775 0.41102842 0.41161304 0.41135044 0.41186999
 0.40913228 0.40920208 0.40973414 0.40764225 0.40783949 0.41001195
 0.40673243 0.40779018 0.40901931 0.40777514 0.40622504 0.40661937
 0.40777514 0.40622504 0.40661937 0.40702633 0.40646027 0.40672681
 0.38057042 0.3805295  0.38132512 0.38064551 0.37979304 0.38141046
 0.3817993  0.37945011 0.37879146 0.37813303 0.37522724 0.37554907
 0.37575063 0.374489   0.37555501 0.3762627  0.3726322  0.37276606
 0.37140237 0.36667809 0.3662066  0.37140237 0.36667809 0.3662066
 0.36979541 0.36549424 0.3658354         nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
 0.31210297 0.29771968 0.29862385 0.30948231 0.29690469 0.29887609
 0.3100976  0.29737537 0.29820366 0.30980529 0.2973846  0.29913017
 0.30930927 0.2972899  0.29937402 0.30980423 0.29777183 0.29849457
 0.31066501 0.29863635 0.29921167 0.31066501 0.29863635 0.29921167
 0.31150985 0.29872803 0.29894328 0.25217743 0.24340565 0.23933996
 0.25344081 0.24411586 0.23873525 0.25146872 0.24220794 0.23698882
 0.25239762 0.24319863 0.23832051 0.25311739 0.24450821 0.23846578
 0.25052292 0.2424212  0.23651032 0.25215801 0.24170388 0.23652248
 0.25215801 0.24170388 0.23652248 0.25144216 0.24240597 0.23660801
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan 0.38468004 0.37998197 0.38190502
 0.38560501 0.38182655 0.38158795 0.38392111 0.38219822 0.38185011
 0.38625435 0.38168655 0.38178682 0.38538064 0.38087432 0.38173991
 0.38712215 0.38219227 0.38214119 0.38421726 0.38071883 0.38084126
 0.38421726 0.38071883 0.38084126 0.38463798 0.38024406 0.38048227
 0.33569939 0.331098   0.32942286 0.3340748  0.32983053 0.32854063
 0.33560027 0.33099764 0.32947312 0.33548979 0.32809012 0.3280414
 0.33456419 0.32821815 0.3274463  0.33451527 0.32860698 0.32665691
 0.33255032 0.32734914 0.32587642 0.33255032 0.32734914 0.32587642
 0.33220676 0.32693916 0.32645641        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
 0.40802661 0.40979544 0.41043517 0.41027001 0.41011464 0.41061676
 0.40937288 0.40923904 0.41039635 0.40787989 0.40724803 0.40904236
 0.40847045 0.40914211 0.4099519  0.40759207 0.40764636 0.40853901
 0.40680878 0.40520647 0.40621339 0.40680878 0.40520647 0.40621339
 0.40649557 0.4052407  0.40571749 0.37545726 0.377486   0.37741104
 0.37688921 0.37633429 0.37643071 0.37754729 0.37515455 0.37529891
 0.37393297 0.37328236 0.37271125 0.37433212 0.37339948 0.37370648
 0.37297334 0.37072551 0.37184147 0.36923067 0.36642661 0.36569875
 0.36923067 0.36642661 0.36569875 0.36757217 0.36395921 0.36482003
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan 0.407982   0.40989902 0.41104755
 0.40995366 0.41164252 0.41318972 0.41211007 0.41251276 0.41377142
 0.41042997 0.41106636 0.41215055 0.41043432 0.41092422 0.41185507
 0.41075477 0.4114403  0.41208462 0.4101085  0.40897476 0.41004719
 0.4101085  0.40897476 0.41004719 0.40955486 0.40903871 0.40987185
 0.38336049 0.3821956  0.38420261 0.38529347 0.38557053 0.38671737
 0.38592372 0.3847927  0.38523364 0.38181119 0.37982161 0.38076849
 0.38208483 0.37999334 0.38129344 0.38252382 0.38030825 0.38028251
 0.37726597 0.3742964  0.37422049 0.37726597 0.3742964  0.37422049
 0.37687321 0.37428771 0.37263242        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
 0.30901828 0.30134963 0.3018213  0.30986329 0.30189038 0.30156537
 0.30872162 0.3010794  0.30121702 0.30987869 0.30133474 0.30142102
 0.30933974 0.30166187 0.30154989 0.30871317 0.30141009 0.30152112
 0.30847077 0.3010322  0.30128505 0.30847077 0.3010322  0.30128505
 0.30892139 0.30143192 0.30148598 0.25401934 0.24210137 0.23837518
 0.25541751 0.24409992 0.2391448  0.25379563 0.24208947 0.23801661
 0.25576496 0.24412849 0.23904528 0.25511687 0.24398711 0.23902061
 0.25395009 0.24205108 0.23805454 0.2515144  0.24162885 0.23753586
 0.2515144  0.24162885 0.23753586 0.25202205 0.24091524 0.23743731
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan 0.38705319 0.38395624 0.38426175
 0.3874656  0.38353827 0.3841111  0.38716105 0.38396738 0.38317944
 0.3855046  0.38344012 0.38333938 0.38451764 0.38287395 0.38356528
 0.38677552 0.38434803 0.38340895 0.38497288 0.38186698 0.38251957
 0.38497288 0.38186698 0.38251957 0.38731498 0.38420779 0.38397202
 0.33896019 0.33137164 0.3311261  0.33939721 0.33306415 0.33124848
 0.33775899 0.33090904 0.33100397 0.3357693  0.33093844 0.33055641
 0.33877312 0.33149252 0.33112277 0.33871538 0.33241544 0.33220945
 0.3364885  0.32949383 0.32924499 0.3364885  0.32949383 0.32924499
 0.33548767 0.32956475 0.3292303         nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
 0.40900878 0.41085107 0.41221492 0.41170675 0.41177761 0.41306123
 0.41135599 0.4118257  0.41313645 0.4090095  0.41006316 0.41205322
 0.41072066 0.41140224 0.41226833 0.41110809 0.41095739 0.4117983
 0.40924667 0.40886587 0.40972411 0.40924667 0.40886587 0.40972411
 0.40925003 0.40826512 0.40922223 0.38140425 0.37945791 0.38088611
 0.38187182 0.37983833 0.38135585 0.38418656 0.38108426 0.38189197
 0.38079041 0.37738778 0.37837549 0.38236499 0.37926202 0.37999753
 0.37958674 0.37662878 0.37688301 0.3763105  0.37263599 0.37118829
 0.3763105  0.37263599 0.37118829 0.37570123 0.37155546 0.37098172]
  warnings.warn(
/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  warnings.warn(
/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: 
1080 fits failed out of a total of 3240.
The score on these train-test partitions for these parameters will be set to nan.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
422 fits failed with the following error:
Traceback (most recent call last):
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/model_selection/_validation.py", line 866, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 1389, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/pipeline.py", line 662, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 1382, in wrapper
    estimator._validate_params()
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 436, in _validate_params
    validate_parameter_constraints(
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 98, in validate_parameter_constraints
    raise InvalidParameterError(
sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.

--------------------------------------------------------------------------------
658 fits failed with the following error:
Traceback (most recent call last):
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/model_selection/_validation.py", line 866, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 1389, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/pipeline.py", line 662, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 1382, in wrapper
    estimator._validate_params()
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 436, in _validate_params
    validate_parameter_constraints(
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 98, in validate_parameter_constraints
    raise InvalidParameterError(
sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.

  warnings.warn(some_fits_failed_message, FitFailedWarning)
/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan 0.41829423 0.4186369  0.42082063
 0.41818169 0.41910692 0.42159522 0.41983695 0.4198131  0.42127197
 0.41698023 0.41862624 0.42021283 0.41815917 0.41873424 0.41922656
 0.41786904 0.41894592 0.41931363 0.4171659  0.41512862 0.41630481
 0.4171659  0.41512862 0.41630481 0.41712977 0.41514506 0.41604695
 0.38913567 0.39068277 0.39298085 0.38984818 0.39035439 0.39174392
 0.38968725 0.38993661 0.38968737 0.38820175 0.38684895 0.38649803
 0.38688167 0.38587851 0.38598417 0.38595854 0.38426083 0.38414396
 0.37965161 0.37682744 0.3763875  0.37965161 0.37682744 0.3763875
 0.38024144 0.37707278 0.37614185        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
 0.31849293 0.30590664 0.30589851 0.31570453 0.30434491 0.30560581
 0.31723065 0.30547377 0.30638966 0.31725516 0.3050552  0.30608254
 0.31631525 0.30465475 0.30575115 0.31802077 0.30602288 0.30641727
 0.31760264 0.30684539 0.30673777 0.31760264 0.30684539 0.30673777
 0.3189736  0.30727736 0.3072728  0.2581358  0.24722141 0.24499759
 0.25808695 0.24658151 0.24429693 0.25928564 0.24757862 0.2438656
 0.25772053 0.2473644  0.24544876 0.2585635  0.24790102 0.24512643
 0.25939194 0.24762202 0.24380031 0.25784879 0.24803423 0.24479939
 0.25784879 0.24803423 0.24479939 0.25936967 0.24787826 0.24433542
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan 0.39355251 0.3908598  0.39204067
 0.39575437 0.39254905 0.39278976 0.39286953 0.38991877 0.39100422
 0.39385331 0.39151056 0.39266158 0.39554491 0.39160287 0.39170254
 0.3964583  0.39202928 0.39245112 0.39546892 0.39175121 0.39225806
 0.39546892 0.39175121 0.39225806 0.39493885 0.39174038 0.39236259
 0.34248935 0.33790967 0.33812488 0.34212263 0.33953356 0.33805931
 0.34502616 0.3411585  0.33916405 0.34339929 0.33855235 0.33737655
 0.34655845 0.33885745 0.33779718 0.34335796 0.33685536 0.33683405
 0.34505593 0.3383924  0.33630173 0.34505593 0.3383924  0.33630173
 0.34278523 0.33758722 0.33649347        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
 0.41879437 0.41925395 0.42003333 0.41626307 0.41741647 0.42015733
 0.41980887 0.41923973 0.42037805 0.41715174 0.4184564  0.41950605
 0.41847296 0.41771629 0.41864049 0.41879309 0.4183587  0.41945782
 0.41717745 0.41501636 0.41586961 0.41717745 0.41501636 0.41586961
 0.41571391 0.41506006 0.41568886 0.38819629 0.38900536 0.38965493
 0.38819186 0.386913   0.38852075 0.38637617 0.38522174 0.38666086
 0.38504004 0.38357836 0.38467903 0.38550055 0.38392289 0.38398786
 0.38483054 0.38170127 0.3817303  0.38116741 0.37720613 0.37549573
 0.38116741 0.37720613 0.37549573 0.37779746 0.37484251 0.37414616
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan 0.41824657 0.42021465 0.42215601
 0.42138222 0.42320746 0.42443718 0.42196597 0.42236345 0.42396989
 0.42136602 0.42142643 0.42235805 0.42034918 0.42116289 0.4221365
 0.42050197 0.42107045 0.4230995  0.42133427 0.41957228 0.42104348
 0.42133427 0.41957228 0.42104348 0.41916126 0.41866116 0.42001923
 0.39078458 0.39386224 0.39567497 0.39596425 0.39597641 0.39646524
 0.39551339 0.39603745 0.3957223  0.39336624 0.39171706 0.39181227
 0.39027998 0.39117512 0.39145287 0.39198656 0.3906865  0.39052276
 0.38355094 0.38280482 0.38300677 0.38355094 0.38280482 0.38300677
 0.38583609 0.38366166 0.38309281        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
 0.31560016 0.30562165 0.30787403 0.31762448 0.30662216 0.30872791
 0.31683624 0.30583417 0.30831871 0.31752342 0.30612237 0.30861943
 0.31790991 0.30634216 0.30895734 0.31725307 0.30503735 0.30823908
 0.31779667 0.30572987 0.30870598 0.31779667 0.30572987 0.30870598
 0.3177761  0.3058731  0.30864889 0.25768062 0.24833577 0.24488024
 0.25811626 0.24847493 0.24408352 0.26024673 0.24887811 0.24419913
 0.25786168 0.24834208 0.24426108 0.2572522  0.24787529 0.24397348
 0.25959276 0.24861105 0.24423419 0.25773081 0.2477377  0.24414629
 0.25773081 0.2477377  0.24414629 0.25932764 0.24788384 0.24420098
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan 0.39650555 0.39418703 0.39427531
 0.39892171 0.39556238 0.39566845 0.39480724 0.39264939 0.39422627
 0.39716245 0.39257313 0.39390598 0.39931056 0.39412185 0.39425894
 0.39565259 0.39299584 0.39472722 0.39544657 0.39338095 0.39391494
 0.39544657 0.39338095 0.39391494 0.39658843 0.39259862 0.39328419
 0.34551466 0.34162874 0.34073521 0.34687604 0.34112984 0.34070895
 0.34647192 0.34013882 0.34039252 0.34713205 0.34132867 0.34080919
 0.34318151 0.33974023 0.34033166 0.34663461 0.34138085 0.34020597
 0.34364241 0.33942527 0.33938365 0.34364241 0.33942527 0.33938365
 0.34539131 0.3402849  0.33976619        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
 0.42004747 0.42089512 0.42234641 0.42251812 0.42215163 0.42309613
 0.42308215 0.4236492  0.42410992 0.41960478 0.42084024 0.42257215
 0.42045815 0.42032776 0.421938   0.42142521 0.42097918 0.42256059
 0.42153414 0.41954219 0.42069917 0.42153414 0.41954219 0.42069917
 0.42070539 0.41952832 0.42029071 0.39354347 0.39261094 0.39306876
 0.39152743 0.39065931 0.39207947 0.39325514 0.39240342 0.39195353
 0.38983309 0.38896384 0.38974173 0.39031894 0.3894146  0.38952446
 0.39131887 0.39035364 0.38818161 0.38554099 0.38309044 0.38237789
 0.38554099 0.38309044 0.38237789 0.38579095 0.382036   0.38163149]
  warnings.warn(
/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  warnings.warn(
/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: 
1080 fits failed out of a total of 3240.
The score on these train-test partitions for these parameters will be set to nan.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
468 fits failed with the following error:
Traceback (most recent call last):
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/model_selection/_validation.py", line 866, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 1389, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/pipeline.py", line 662, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 1382, in wrapper
    estimator._validate_params()
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 436, in _validate_params
    validate_parameter_constraints(
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 98, in validate_parameter_constraints
    raise InvalidParameterError(
sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.

--------------------------------------------------------------------------------
612 fits failed with the following error:
Traceback (most recent call last):
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/model_selection/_validation.py", line 866, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 1389, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/pipeline.py", line 662, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 1382, in wrapper
    estimator._validate_params()
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 436, in _validate_params
    validate_parameter_constraints(
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 98, in validate_parameter_constraints
    raise InvalidParameterError(
sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.

  warnings.warn(some_fits_failed_message, FitFailedWarning)
/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan 0.41148231 0.41300577 0.41589769
 0.41509471 0.41527959 0.41698804 0.41507777 0.41430172 0.41592677
 0.41323796 0.41381906 0.41534876 0.41599635 0.41502945 0.41539991
 0.41487263 0.41469512 0.41492823 0.41218284 0.41090506 0.41196563
 0.41218284 0.41090506 0.41196563 0.41289068 0.41064176 0.4107929
 0.38925812 0.38712111 0.3883876  0.38755877 0.38735945 0.38786183
 0.38612997 0.38583046 0.38571748 0.38242903 0.38088757 0.38238915
 0.38230612 0.38191572 0.38255785 0.38114826 0.37926977 0.37968811
 0.37553409 0.37191775 0.37237331 0.37553409 0.37191775 0.37237331
 0.37524636 0.37260017 0.37205957        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
 0.31615371 0.30483868 0.30433475 0.31664762 0.30488154 0.3039157
 0.31711771 0.30501552 0.3031153  0.31665351 0.30376464 0.30348914
 0.31699067 0.30429522 0.30395846 0.31673984 0.3047371  0.30348445
 0.31797164 0.30512322 0.30377524 0.31797164 0.30512322 0.30377524
 0.31772365 0.3055833  0.30379584 0.25113127 0.2424451  0.23948033
 0.25160807 0.24158848 0.23850573 0.25148805 0.24152083 0.23841769
 0.25249495 0.24253522 0.23938932 0.25204772 0.24191938 0.23895926
 0.25210252 0.2413171  0.23848025 0.25274283 0.2423284  0.24023147
 0.25274283 0.2423284  0.24023147 0.25153492 0.24174001 0.23959539
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan 0.39056952 0.38650844 0.38670215
 0.39128597 0.38750871 0.38730732 0.39496288 0.389588   0.38854709
 0.39038179 0.38623031 0.38791596 0.39230075 0.38818896 0.38674953
 0.39189586 0.38688176 0.38695753 0.39110717 0.38654439 0.3859294
 0.39110717 0.38654439 0.3859294  0.38987774 0.38573265 0.38550347
 0.34061812 0.33563751 0.33480996 0.34153469 0.33403272 0.33379164
 0.34251169 0.33616909 0.33426509 0.34040138 0.33440634 0.33273168
 0.33584956 0.33322675 0.33350193 0.33982198 0.33404161 0.3325896
 0.33805442 0.33235715 0.33180267 0.33805442 0.33235715 0.33180267
 0.33667783 0.33096785 0.33081073        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
 0.4158531  0.41495819 0.41611673 0.41610018 0.41632967 0.41638513
 0.41419321 0.41402288 0.41515191 0.41521659 0.41461173 0.41462811
 0.41360473 0.4141013  0.41495903 0.41426364 0.41364607 0.41415851
 0.41283817 0.41117896 0.41218708 0.41283817 0.41117896 0.41218708
 0.41201299 0.41040747 0.41144474 0.38305813 0.38343401 0.38485905
 0.38291338 0.38240552 0.38359366 0.38215768 0.38139037 0.38134291
 0.38073316 0.37940159 0.38030526 0.38093294 0.37987531 0.38001326
 0.38090086 0.37843125 0.37791576 0.37427746 0.37161218 0.3716331
 0.37427746 0.37161218 0.3716331  0.37455641 0.3711556  0.37103234
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan 0.41201778 0.41479288 0.41682073
 0.41681414 0.41675376 0.4175669  0.4167271  0.41714873 0.4191994
 0.41526714 0.41584982 0.41670004 0.41522782 0.41614705 0.41624519
 0.41745891 0.41734656 0.41795277 0.41612893 0.41452074 0.41509492
 0.41612893 0.41452074 0.41509492 0.41532801 0.41359547 0.41456487
 0.38892017 0.39039083 0.39087109 0.38896313 0.38977587 0.39109875
 0.39425365 0.39126915 0.39132235 0.38897795 0.38657214 0.38692169
 0.38885158 0.38640768 0.38712222 0.38790578 0.38559306 0.38616366
 0.38133081 0.3798804  0.37922214 0.38133081 0.3798804  0.37922214
 0.38115666 0.3784371  0.37888032        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
 0.31297901 0.30424979 0.30342998 0.31305072 0.30395128 0.30252206
 0.31336845 0.30383508 0.30243339 0.31409807 0.30514453 0.30361621
 0.31348544 0.30455848 0.30319584 0.31428882 0.30454926 0.30299705
 0.31393121 0.30408563 0.30318932 0.31393121 0.30408563 0.30318932
 0.31441309 0.30414527 0.30321727 0.25336321 0.24230574 0.23923121
 0.25444504 0.24303152 0.23943905 0.25641739 0.24378961 0.2392131
 0.25382451 0.24247872 0.24015755 0.25480064 0.2431477  0.23953116
 0.2566675  0.24379814 0.23934052 0.25517838 0.24382122 0.23937368
 0.25517838 0.24382122 0.23937368 0.25603816 0.24410513 0.23988865
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan 0.39308521 0.38941993 0.38859752
 0.39183576 0.38775889 0.38791744 0.39196783 0.38872108 0.38878504
 0.39373216 0.39009181 0.38983654 0.39178328 0.38858875 0.38874294
 0.39262378 0.38795926 0.38917502 0.39323309 0.38929541 0.38819677
 0.39323309 0.38929541 0.38819677 0.3929987  0.38743395 0.38830171
 0.34317133 0.3367599  0.33653906 0.34262946 0.33626483 0.3360945
 0.34504406 0.33600073 0.33555063 0.34293548 0.33619624 0.33580021
 0.34381324 0.33764225 0.33584105 0.34297148 0.33605447 0.33527341
 0.34486976 0.3359293  0.33401945 0.34486976 0.3359293  0.33401945
 0.34168293 0.33580587 0.33426318        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
 0.41436762 0.41388228 0.41580943 0.41609476 0.41589879 0.41777986
 0.41776454 0.41733917 0.41862518 0.41663192 0.4156567  0.41625515
 0.41537499 0.41469582 0.4167617  0.418193   0.41742774 0.41741315
 0.41470388 0.41387296 0.41418106 0.41470388 0.41387296 0.41418106
 0.41513488 0.41498464 0.41438376 0.38607033 0.38564652 0.38713738
 0.38844507 0.38702991 0.38752436 0.38732225 0.38557473 0.38624408
 0.38711834 0.38462887 0.38451931 0.38764205 0.38458747 0.38377858
 0.38358198 0.38157092 0.38257786 0.37957636 0.3774273  0.37784432
 0.37957636 0.3774273  0.37784432 0.38018862 0.37715903 0.37745785]
  warnings.warn(
/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  warnings.warn(
/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: 
1080 fits failed out of a total of 3240.
The score on these train-test partitions for these parameters will be set to nan.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
494 fits failed with the following error:
Traceback (most recent call last):
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/model_selection/_validation.py", line 866, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 1389, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/pipeline.py", line 662, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 1382, in wrapper
    estimator._validate_params()
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 436, in _validate_params
    validate_parameter_constraints(
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 98, in validate_parameter_constraints
    raise InvalidParameterError(
sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.

--------------------------------------------------------------------------------
586 fits failed with the following error:
Traceback (most recent call last):
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/model_selection/_validation.py", line 866, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 1389, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/pipeline.py", line 662, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 1382, in wrapper
    estimator._validate_params()
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/base.py", line 436, in _validate_params
    validate_parameter_constraints(
  File "/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 98, in validate_parameter_constraints
    raise InvalidParameterError(
sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.

  warnings.warn(some_fits_failed_message, FitFailedWarning)
/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan 0.41686885 0.41817698 0.41952264
 0.41898522 0.41974534 0.4210557  0.4203679  0.42043009 0.42091674
 0.41707922 0.41789473 0.41946819 0.41924518 0.41878572 0.42005896
 0.4190016  0.41807566 0.4189232  0.41637945 0.41607649 0.41663753
 0.41637945 0.41607649 0.41663753 0.41598495 0.41386082 0.41432998
 0.38862553 0.39038676 0.39222559 0.3915677  0.39080091 0.39136037
 0.39102767 0.38955007 0.38936102 0.38622093 0.38512611 0.38533887
 0.38242163 0.38276526 0.38333287 0.38239329 0.38134803 0.38201543
 0.37778462 0.37605554 0.37538412 0.37778462 0.37605554 0.37538412
 0.37684531 0.37469834 0.3745967         nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
 0.31345583 0.30310287 0.30291037 0.31469098 0.30424287 0.30363701
 0.31215647 0.30391965 0.30361721 0.31230892 0.30202816 0.30311865
 0.31381496 0.30267947 0.30325533 0.31081528 0.30265833 0.30317237
 0.31156395 0.30242475 0.30285572 0.31156395 0.30242475 0.30285572
 0.30992485 0.30180702 0.30256967 0.25652277 0.2483558  0.2428352
 0.25758801 0.24812459 0.24245253 0.25486545 0.24715934 0.24258631
 0.25799199 0.24830673 0.24280758 0.25846788 0.24853107 0.24283754
 0.25584978 0.24754353 0.24266131 0.25646767 0.24817846 0.24293884
 0.25646767 0.24817846 0.24293884 0.25645582 0.24844408 0.24291377
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan 0.39420942 0.39225196 0.39128849
 0.39396292 0.38992871 0.39054561 0.39257881 0.38990151 0.39051132
 0.39429268 0.3900902  0.38974485 0.39344545 0.39019067 0.38985741
 0.39375771 0.39107336 0.39092771 0.39401594 0.39008852 0.38922201
 0.39401594 0.39008852 0.38922201 0.39487232 0.390219   0.38971036
 0.34351953 0.33724699 0.33609707 0.34245603 0.33738963 0.33614015
 0.34026456 0.3347559  0.33463112 0.34518705 0.33686163 0.33610586
 0.33916753 0.3353294  0.33541124 0.34257443 0.3381166  0.33601751
 0.34034463 0.33565927 0.33478851 0.34034463 0.33565927 0.33478851
 0.34114723 0.33563513 0.3357492         nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
 0.41803446 0.41964587 0.42028249 0.41651901 0.41777094 0.41991138
 0.42004515 0.4186762  0.41957089 0.41931982 0.4185802  0.4190946
 0.41714014 0.41788858 0.41865751 0.41707949 0.41576284 0.41707461
 0.41592822 0.41501818 0.41541488 0.41592822 0.41501818 0.41541488
 0.41743468 0.41467236 0.41449816 0.38975603 0.38869468 0.38843791
 0.38720421 0.38676226 0.38756068 0.38353894 0.38290904 0.38471308
 0.38341438 0.38229858 0.38263343 0.38351085 0.38207927 0.38299946
 0.38180816 0.380592   0.38092908 0.37712902 0.37612746 0.37517907
 0.37712902 0.37612746 0.37517907 0.3756538  0.37214718 0.37245228
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan 0.42012225 0.4199149  0.42209623
 0.42103291 0.42170511 0.4232903  0.42305889 0.42157431 0.42315343
 0.42105071 0.42159916 0.4220051  0.420829   0.42098157 0.42148809
 0.42262338 0.42149683 0.42230005 0.42009776 0.41919828 0.42001841
 0.42009776 0.41919828 0.42001841 0.42044817 0.4194904  0.41983476
 0.39112222 0.39272664 0.39470871 0.39736269 0.39511621 0.39666297
 0.39511046 0.39565512 0.3960179  0.39053292 0.39041302 0.39159818
 0.39106735 0.39063183 0.39195397 0.3917179  0.38935545 0.38982746
 0.38520807 0.38325529 0.3829852  0.38520807 0.38325529 0.3829852
 0.38423687 0.38211147 0.38221287        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
 0.31516538 0.30325243 0.30274305 0.31463159 0.30370308 0.30327553
 0.31406717 0.30326155 0.3033395  0.31482    0.30416594 0.3033682
 0.31473313 0.30424039 0.30340384 0.31356441 0.30277823 0.30298364
 0.31549    0.30355598 0.30293123 0.31549    0.30355598 0.30293123
 0.31432671 0.30239725 0.30251293 0.25577208 0.24508392 0.24207706
 0.25639889 0.24528183 0.24203425 0.25580845 0.24439422 0.24149499
 0.2567675  0.24643544 0.24283353 0.25680639 0.24563861 0.24263217
 0.25626192 0.245024   0.24165407 0.25552022 0.24426184 0.24215369
 0.25552022 0.24426184 0.24215369 0.25524175 0.24380158 0.24153062
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan 0.3978781  0.39319852 0.39355558
 0.39744891 0.39348312 0.39282508 0.39755199 0.39259435 0.39318597
 0.39654867 0.39336779 0.39311848 0.39757127 0.39373146 0.39335026
 0.39719702 0.39235404 0.39284259 0.39492275 0.39185927 0.39184666
 0.39492275 0.39185927 0.39184666 0.39519384 0.39207547 0.39138096
 0.34198685 0.33724257 0.33770648 0.34403405 0.34070944 0.33875274
 0.3432165  0.33937037 0.33824077 0.34180841 0.33978621 0.33930603
 0.34580556 0.33925331 0.33933846 0.34523526 0.33976137 0.33854723
 0.34266571 0.33804629 0.33802768 0.34266571 0.33804629 0.33802768
 0.34261843 0.33868703 0.33829056        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
 0.42154037 0.42138571 0.42284631 0.42042867 0.42123894 0.42313666
 0.42347843 0.42287816 0.42314261 0.42204441 0.42150575 0.42230812
 0.42119309 0.42033405 0.4217308  0.42096946 0.42081819 0.42202766
 0.41978515 0.41841337 0.41988515 0.41978515 0.41841337 0.41988515
 0.41951631 0.41785094 0.41936679 0.39174936 0.39071663 0.39201255
 0.39279344 0.39161661 0.39259622 0.39244293 0.39012432 0.39101092
 0.38935015 0.38815822 0.38873616 0.39005917 0.38798559 0.38836041
 0.38898468 0.38792051 0.38810405 0.38442958 0.38294703 0.38181449
 0.38442958 0.38294703 0.38181449 0.38432218 0.38281484 0.38125428]
  warnings.warn(
/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  warnings.warn(
/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  warnings.warn(
/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  warnings.warn(
/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  warnings.warn(
/rds/general/user/hsl121/home/anaconda3/envs/TDS/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  warnings.warn(
